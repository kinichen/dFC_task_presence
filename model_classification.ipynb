{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from contrastive import CPCA\t# $ pip3 install contrastive\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# set for your own directory\n",
    "my_filepath = \"/Users/kinichen/Summer_dFC/Datasets/ds003465_task-Axcpt_Time-Freq.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_reduction_train(X, model_class, n_components, Y=None, alpha=1):\n",
    "\t\"\"\"\n",
    "\tFit dimension reduction pipeline on train data, and return both transformed X and the reusable pipeline.\n",
    "\t\n",
    "\tParameters:\n",
    "\t\tX: array-like, shape (n_samples_train, n_features)\n",
    "\t\tmodel_class: class (PCA, CCA, CPCA, or UMAP)\n",
    "\t\tn_components: number of components to keep on outermost layer (not PCA if\n",
    "\t\t\tit is used as a preprocessing step)\n",
    "\t\tY: for supervised CCA or CPCA background\n",
    "\t\talpha: CPCA contrastive parameter\n",
    "\t\t\n",
    "\tReturns:\n",
    "\t\tX_reduced: transformed training data\n",
    "\t\tpipeline: object to reuse to transform test data\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tscaler_X = StandardScaler()\n",
    "\tX_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "\tif model_class.__name__ == 'PCA':\n",
    "\t\tmodel = model_class(n_components=n_components)\n",
    "\t\tpipeline = Pipeline([\n",
    "\t\t\t('scaler', scaler_X),\n",
    "\t\t\t('pca', model)\n",
    "\t\t])\n",
    "\t\tX_reduced = pipeline.fit_transform(X)\n",
    "\n",
    "\telif model_class.__name__ == 'CCA':\n",
    "\t\t# Note: it only makes sense to use supervised CCA with labels Y=y. If\n",
    "\t\t# using Y=X_rest, the reduced subspace does not enhance task-specific\n",
    "\t\t# features, but instead returns the shared subspace between task and rest.\n",
    "\t\tif Y is None:\n",
    "\t\t\traise ValueError(\"CCA requires supervised labels Y.\")\n",
    "\n",
    "\t\t# Preprocess labels into \"2D\" array with shape (n_samples, 1), then OneHotEncoder\n",
    "\t\ty = OneHotEncoder(sparse_output=False).fit_transform(Y.reshape(-1, 1))\n",
    "\n",
    "\t\t# PCA before CCA\n",
    "\t\tpca = PCA(n_components=1000)\n",
    "\t\tX_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\t\tcca = model_class(n_components=n_components)\n",
    "\t\tX_cca, _ = cca.fit_transform(X_pca, y)\n",
    "\n",
    "\t\t# Store fitted pipeline parts manually into a dictionary\n",
    "\t\tpipeline = {\n",
    "\t\t\t'scaler': scaler_X,\n",
    "\t\t\t'pca': pca,\n",
    "\t\t\t'cca': cca,\n",
    "\t\t\t'label_encoder': y  # just for reference if needed\n",
    "\t\t}\n",
    "\t\tX_reduced = X_cca\n",
    "\n",
    "\telif model_class.__name__ == 'CPCA':\n",
    "\t\tif Y is None:\n",
    "\t\t\traise ValueError(\"CPCA requires background dataset Y.\")\n",
    "\t\t\n",
    "\t\tscaler_Y = StandardScaler()\n",
    "\t\tY_scaled = scaler_Y.fit_transform(Y)\n",
    "\n",
    "\t\t# Note: the CPCA class intrinsically applies PCA to reduce to 1000 \n",
    "  \t\t# components first, so for consistency on test data, do this explicitly\n",
    "\t\tpca = PCA(n_components=1000)\n",
    "\t\tX_pca = pca.fit_transform(X_scaled)\n",
    "\t\tY_pca = pca.transform(Y_scaled)\t# transform Y into the same PCA feature space\n",
    "\n",
    "\t\tcpca = model_class(n_components=n_components)\n",
    "\t\tX_cpca = cpca.fit_transform(X_pca, Y_pca, \n",
    "\t\t\t\t\t\t\talpha_selection='manual', alpha_value=alpha)\n",
    "\t\t\n",
    "\t\tpipeline = {\n",
    "\t\t\t'scaler_X': scaler_X,\t# Don't store scaler_Y, as it is not needed for test data\n",
    "\t\t\t'pca': pca,\n",
    "   \t\t\t'cpca': cpca,\n",
    "\t\t\t'alpha': alpha \n",
    "\t\t}\n",
    "\t\tX_reduced = X_cpca\n",
    "\n",
    "\telif model_class.__name__ == 'UMAP':\n",
    "\t\tpca = PCA(n_components=1000)\n",
    "\t\tX_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\t\tumap_model = umap.UMAP(n_neighbors=50, min_dist=0.1, \n",
    "\t\t\t\t\t\t\t   n_components=n_components, random_state=25)\n",
    "\t\tX_umap = umap_model.fit_transform(X_pca)\n",
    "\n",
    "\t\tpipeline = {\n",
    "\t\t\t'scaler': scaler_X,\n",
    "\t\t\t'pca': pca,\n",
    "\t\t\t'umap': umap_model\n",
    "\t\t}\n",
    "\t\tX_reduced = X_umap\n",
    "\n",
    "\telse:\n",
    "\t\traise ValueError(\"Only PCA, CCA, CPCA, and UMAP are supported right now.\")\n",
    "\n",
    "\treturn X_reduced, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_reduction_test(X, pipeline):\n",
    "\t\"\"\"\n",
    "\tReturn the transformed test data using the fitted model pipeline.\n",
    "\t\n",
    "\tParameters:\n",
    "\t\tX: array-like, shape (n_samples_test, n_features)\n",
    "\t\tpipeline: fitted model pipeline (PCA, CCA, CPCA, or UMAP)\n",
    "\t\tY: optional labels (needed for supervised CCA or CPCA background)\n",
    "\t\t\n",
    "\tReturns:\n",
    "\t\tX_reduced: transformed test data\n",
    "\t\"\"\"\n",
    "\tif isinstance(pipeline, Pipeline):  # only pure PCA used a Pipeline object\n",
    "\t\tX_reduced = pipeline.transform(X)\n",
    "\t\t\n",
    "\telif 'cca' in pipeline:\n",
    "\t\tX_scaled = pipeline['scaler'].transform(X)\n",
    "\t\tX_pca = pipeline['pca'].transform(X_scaled)\n",
    "\t\tX_reduced = pipeline['cca'].transform(X_pca)\n",
    "\n",
    "\telif 'cpca' in pipeline:\n",
    "\t\tX_scaled = pipeline['scaler_X'].transform(X)\n",
    "\t\tX_pca = pipeline['pca'].transform(X_scaled)\n",
    "\t\tX_reduced = pipeline['cpca'].transform(X_pca,\n",
    "\t\t\t\t\t\talpha_selection='manual', alpha_value=pipeline['alpha'])\n",
    "\n",
    "\telif 'umap' in pipeline:\n",
    "\t\tX_scaled = pipeline['scaler'].transform(X)\n",
    "\t\tX_pca = pipeline['pca'].transform(X_scaled)\n",
    "\t\tX_reduced = pipeline['umap'].transform(X_pca)\n",
    "\n",
    "\telse:\n",
    "\t\traise ValueError(\"Only PCA, CCA, CPCA and UMAP are supported right now.\")\n",
    "\t\n",
    "\treturn X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(classifier, X_train, y_train, X_test, y_test):\n",
    "\t\"\"\"\n",
    "\tEvaluate the performance of a fitted classifier on train and test data.\n",
    "\t\"\"\"\n",
    "\ty_train_pred = classifier.predict(X_train)\t# binary\n",
    "\ty_train_prob = classifier.predict_proba(X_train)[:, 1]\t# probabilities of task\n",
    "\ty_test_pred = classifier.predict(X_test)\n",
    "\ty_test_prob = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\tprint(\"Train accuracy:\", round(accuracy_score(y_train, y_train_pred), 4))\n",
    "\tprint(\"Train AUC:\", round(roc_auc_score(y_train, y_train_prob), 4))\n",
    "\tprint(\"Test accuracy:\", round(accuracy_score(y_test, y_test_pred), 4))\t# threshold decisions\n",
    "\t# since two classes, accuracy of 0.5 = random guess\n",
    "\tprint(\"Test AUC:\", round(roc_auc_score(y_test, y_test_prob), 4))\t# evaluates how well the\n",
    "\t# model predicts the probability of the positive class (1 = task). \n",
    " \t# for this metric, 0.5 = random guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset for 1 task paradigm assessed by 1 method for all subjects (1 run)\n",
    "dFC = np.load(my_filepath, allow_pickle=True)\n",
    "dFC_dict = dFC.item() # extract the dictionary from np array\n",
    "\n",
    "X = dFC_dict[\"X\"]\n",
    "y = dFC_dict[\"y\"]\n",
    "subj_label = dFC_dict[\"subj_label\"]\n",
    "method = dFC_dict[\"measure_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.5874\n",
      "Train AUC: 0.571\n",
      "Test accuracy: 0.6014\n",
      "Test AUC: 0.5824\n"
     ]
    }
   ],
   "source": [
    "# PCA + Logistic Regression with penalty\n",
    "\n",
    "# First, split!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Dimensionality reduction\n",
    "# Note: to be comparable, all dim reduction methods should keep the same number \n",
    "# of components at the end. Only 2 are kept since supervised CCA can only keep a maximum of 2.\n",
    "X_train_reduced, pipeline = dim_reduction_train(X_train, PCA, n_components=2)\n",
    "X_test_reduced = dim_reduction_test(X_test, pipeline)\t# transform test data for later\n",
    "\n",
    "# Classification model training\n",
    "classifier = LogisticRegression(penalty='l1', solver='saga', max_iter=5000)\n",
    "classifier.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluate_performance(classifier, X_train_reduced, y_train, X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.6371\n",
      "Train AUC: 0.6684\n",
      "Test accuracy: 0.621\n",
      "Test AUC: 0.6378\n"
     ]
    }
   ],
   "source": [
    "# PCA + Logistic Regression with penalty WITH 100 COMPONENTS\n",
    "\n",
    "# First, split!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Dimensionality reduction\n",
    "# Note: to be comparable, all dim reduction methods should keep the same number of components at the end\n",
    "X_train_reduced, pipeline = dim_reduction_train(X_train, PCA, n_components=100)\n",
    "X_test_reduced = dim_reduction_test(X_test, pipeline)\t# transform test data for later\n",
    "\n",
    "# Classification model training\n",
    "classifier = LogisticRegression(penalty='l1', solver='saga', max_iter=5000)\n",
    "classifier.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluate_performance(classifier, X_train_reduced, y_train, X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7468\n",
      "Train AUC: 0.8271\n",
      "Test accuracy: 0.6861\n",
      "Test AUC: 0.7366\n"
     ]
    }
   ],
   "source": [
    "# Supervised CCA + Logistic Regression with penalty\n",
    "\n",
    "# First, split!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Dimensionality reduction\t(after one-hot encoding the labels, max 2 canonical components)\n",
    "X_train_reduced, pipeline = dim_reduction_train(X_train, CCA, n_components=2, Y=y_train)\n",
    "X_test_reduced = dim_reduction_test(X_test, pipeline)\t# transform test data for later\n",
    "\n",
    "# Classification model training\n",
    "classifier = LogisticRegression(penalty='l1', solver='saga', max_iter=5000)\n",
    "classifier.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluate_performance(classifier, X_train_reduced, y_train, X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.5381\n",
      "Train AUC: 0.5102\n",
      "Test accuracy: 0.5264\n",
      "Test AUC: 0.5289\n"
     ]
    }
   ],
   "source": [
    "# CPCA + Logistic Regression with penalty\n",
    "\n",
    "# First, split!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Dimensionality reduction\n",
    "X_train_rest = X_train[y_train == 0]  # background dataset for cPCA\n",
    "X_train_reduced, pipeline = dim_reduction_train(X_train, CPCA, n_components=2, \n",
    "                                                Y=X_train_rest, alpha=0.5)\n",
    "X_test_reduced = dim_reduction_test(X_test, pipeline)   # project test data on \n",
    "# learned subspace, so don't provide background Y=X_test_rest or else data leakage\n",
    "\n",
    "# Classification model training\n",
    "classifier = LogisticRegression(penalty='l1', solver='saga', max_iter=5000)\n",
    "classifier.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluate_performance(classifier, X_train_reduced, y_train, X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.5625\n",
      "Train AUC: 0.6293\n",
      "Test accuracy: 0.5657\n",
      "Test AUC: 0.5891\n"
     ]
    }
   ],
   "source": [
    "# CPCA + Logistic Regression with penalty WITH 100 COMPONENTS\n",
    "\n",
    "# First, split!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Dimensionality reduction\n",
    "X_train_rest = X_train[y_train == 0]  # background dataset for cPCA\n",
    "X_train_reduced, pipeline = dim_reduction_train(X_train, CPCA, n_components=100, \n",
    "                                                Y=X_train_rest, alpha=0.5)\n",
    "X_test_reduced = dim_reduction_test(X_test, pipeline)   # project test data on \n",
    "# learned subspace, so don't provide background Y=X_test_rest or else data leakage\n",
    "\n",
    "# Classification model training\n",
    "classifier = LogisticRegression(penalty='l1', solver='saga', max_iter=5000)\n",
    "classifier.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluate_performance(classifier, X_train_reduced, y_train, X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinichen/miniconda3/envs/neuro_jb_lab_env/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.6053\n",
      "Train AUC: 0.5729\n",
      "Test accuracy: 0.6218\n",
      "Test AUC: 0.5937\n"
     ]
    }
   ],
   "source": [
    "# UMAP + Logistic Regression with penalty\n",
    "\n",
    "# First, split!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Dimensionality reduction\n",
    "X_train_reduced, pipeline = dim_reduction_train(X_train, umap.UMAP, n_components=2)\n",
    "X_test_reduced = dim_reduction_test(X_test, pipeline)\n",
    "\n",
    "# Classification model training\n",
    "classifier = LogisticRegression(penalty='l1', solver='saga', max_iter=5000)\n",
    "classifier.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Evaluation metrics\n",
    "evaluate_performance(classifier, X_train_reduced, y_train, X_test_reduced, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_jb_lab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
